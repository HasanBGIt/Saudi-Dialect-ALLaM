{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5f8f35d",
   "metadata": {},
   "source": [
    "## Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af02bf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import re, json, csv, numpy as np\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import torch, sacrebleu\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from bert_score import score as bertscore\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import os, re, json, random, csv\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import torch, sacrebleu\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification\n",
    ")\n",
    "from peft import PeftModel\n",
    "from bert_score import score as bertscore\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88fdc7d",
   "metadata": {},
   "source": [
    "## Logging to Hugging Face\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3304864a-1393-4bbf-85ea-1c7621338e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "login(\"TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73a3d39",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c124e572-2c56-49b6-88a6-f33f57a99a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"inceptionai/jais-13b-chat\"   \n",
    "OUT_DIR  = Path(\"eval_external_jais_13b\"); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PRED_DIR = OUT_DIR / \"preds\"; PRED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TEST_PATH = Path(\"data_splits/test.jsonl\")\n",
    "GEN_KW = dict(max_new_tokens=256, do_sample=True, top_p=0.95, top_k=50, temperature=0.6)\n",
    "USE_4BIT = False  \n",
    "SEED = 42\n",
    "USE_4BIT = True  # 13B often needs int4 on a single GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e7a18c",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ef490d-a74b-490d-bfa1-59e2c296beae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "836"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(SEED); np.random.seed(SEED); \n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def read_jsonl(p):\n",
    "    rows=[]\n",
    "    with p.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip(): rows.append(json.loads(line))\n",
    "    return rows\n",
    "\n",
    "test_rows = read_jsonl(TEST_PATH)\n",
    "len(test_rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6419bf3a",
   "metadata": {},
   "source": [
    "## Generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd51d0c-7bf2-42a9-9c71-065ca49623e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Downloading shards: 100%|██████████| 6/6 [03:13<00:00, 32.26s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 6/6 [01:13<00:00, 12.19s/it]\n",
      "/workspace/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Generating: jais-13b-chat:   0%|          | 0/836 [00:00<?, ?it/s]/workspace/.venv/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:456: UserWarning: Some matrices hidden dimension is not a multiple of 64 and efficient inference kernels are not supported for these (slow). Matrix input size found: torch.Size([1, 1, 13653])\n",
      "  warn(\n",
      "Generating: jais-13b-chat: 100%|██████████| 836/836 [57:58<00:00,  4.16s/it]  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('eval_external_jais_13b/preds/jais-13b-chat.jsonl')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def apply_chat_template_safe(tok, user_text):\n",
    "    if hasattr(tok, \"apply_chat_template\") and tok.chat_template:\n",
    "        msgs=[{\"role\":\"user\",\"content\":user_text}]\n",
    "        return tok.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n",
    "    return f\"### Instruction:\\n{user_text}\\n\\n### Response:\\n\"\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "USE_4BIT = True  \n",
    "\n",
    "def load_model(model_id: str):\n",
    "    tok = AutoTokenizer.from_pretrained(\n",
    "        model_id,\n",
    "        use_fast=True,\n",
    "        trust_remote_code=True,\n",
    "        resume_download=True,\n",
    "    )\n",
    "    if tok.pad_token is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "\n",
    "    torch_dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "    kwargs = dict(\n",
    "        torch_dtype=torch_dtype,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True, \n",
    "    )\n",
    "\n",
    "    if USE_4BIT:\n",
    "        kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float16,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "        )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, resume_download=True,**kwargs).eval()\n",
    "    return tok, model\n",
    "\n",
    "\n",
    "tok, model = load_model(MODEL_ID)\n",
    "\n",
    "preds=[]\n",
    "for ex in tqdm(test_rows, desc=f\"Generating: {MODEL_ID.split('/')[-1]}\"):\n",
    "    instr = ex.get(\"instruction\",\"\")\n",
    "    prompt = apply_chat_template_safe(tok, instr)\n",
    "    inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(**inputs, **GEN_KW, eos_token_id=tok.eos_token_id)\n",
    "    txt = tok.decode(out[0], skip_special_tokens=True)\n",
    "    resp = txt.split(\"### Response:\",1)[-1].strip() if \"### Response:\" in txt else txt.strip()\n",
    "    preds.append({\n",
    "        \"prompt\": instr, \"gold\": ex.get(\"response\",\"\"), \"pred\": resp,\n",
    "        \"target_dialect\": (ex.get(\"dialect\") or ex.get(\"meta\",{}).get(\"dialect\") or \"\").strip()\n",
    "    })\n",
    "\n",
    "out_path = PRED_DIR / f\"{MODEL_ID.split('/')[-1]}.jsonl\"\n",
    "with out_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for r in preds: f.write(json.dumps(r, ensure_ascii=False)+\"\\n\")\n",
    "out_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63831b36",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62b7f44-3318-4ef7-a5f9-26839d1ba7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR  = Path(\"eval_external_jais_13b\")\n",
    "PRED_DIR = OUT_DIR / \"preds\"\n",
    "pred_files = [out_path]   \n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ---------- load 5-way written dialect classifier ----------\n",
    "# Classes: typically {MAGH, LEV, MSA, GLF, EGY}\n",
    "DID_ID   = \"IbrahimAmin/marbertv2-arabic-written-dialect-classifier\"\n",
    "did_tok  = AutoTokenizer.from_pretrained(DID_ID, use_fast=True)\n",
    "did_model= AutoModelForSequenceClassification.from_pretrained(DID_ID).to(device).eval()\n",
    "id2label = did_model.config.id2label\n",
    "label2id = { (v if isinstance(v,str) else v.get(\"name\",\"\")).upper(): int(k) for k,v in id2label.items() }\n",
    "\n",
    "def read_jsonl(p: Path):\n",
    "    rows=[]\n",
    "    with p.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip(): rows.append(json.loads(line))\n",
    "    return rows\n",
    "\n",
    "def camel_probs_batched(texts, bs=64):\n",
    "    all_probs=[]\n",
    "    for i in tqdm(range(0, len(texts), bs), desc=\"Dialect scoring (MARBERTv2)\", leave=False):\n",
    "        chunk = texts[i:i+bs]\n",
    "        with torch.no_grad():\n",
    "            batch = did_tok(chunk, padding=True, truncation=True, max_length=128, return_tensors=\"pt\").to(device)\n",
    "            logits = did_model(**batch).logits\n",
    "            probs = torch.softmax(logits, dim=-1).cpu().numpy()\n",
    "            all_probs.append(probs)\n",
    "    return np.vstack(all_probs) if all_probs else np.zeros((0, len(id2label)))\n",
    "\n",
    "def labels_from_probs(probs):\n",
    "    ids = probs.argmax(axis=1)\n",
    "    out=[]\n",
    "    for i in ids:\n",
    "        key = str(i)\n",
    "        lab = id2label[key] if key in id2label else id2label[i]\n",
    "        lab = lab if isinstance(lab, str) else lab.get(\"name\",\"\")\n",
    "        out.append(lab.upper().strip())\n",
    "    return out\n",
    "\n",
    "def is_saudi(lbl: str) -> bool:\n",
    "    # GLF == Gulf Arabic (counts as Saudi-region for this eval)\n",
    "    return \"GLF\" in lbl\n",
    "\n",
    "# MSA class index\n",
    "MSA_IDX = None\n",
    "for raw, idx in label2id.items():\n",
    "    if \"MSA\" in raw:\n",
    "        MSA_IDX = idx\n",
    "        break\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def tag_echo_rate(texts):\n",
    "    patt = re.compile(r'<\\s*DIALECT\\s*=\\s*[^>]+>', re.IGNORECASE)\n",
    "    return 100.0 * float(np.mean([bool(patt.search(t)) for t in texts]))\n",
    "\n",
    "def diversity_metrics(preds):\n",
    "    def ngrams(tokens, n): return list(zip(*[tokens[i:] for i in range(n)]))\n",
    "    total_bi=total_tri=0; uniq_bi=set(); uniq_tri=set()\n",
    "    for p in preds:\n",
    "        t=p.split()\n",
    "        b=ngrams(t,2); g=ngrams(t,3)\n",
    "        total_bi += max(1,len(b)); total_tri += max(1,len(g))\n",
    "        uniq_bi.update(b); uniq_tri.update(g)\n",
    "    d2 = len(uniq_bi)/total_bi if total_bi else 0.0\n",
    "    d3 = len(uniq_tri)/total_tri if total_tri else 0.0\n",
    "    if len(preds) < 2:\n",
    "        sbleu = 0.0\n",
    "    else:\n",
    "        scores=[]\n",
    "        for i in range(len(preds)):\n",
    "            hyp=[preds[i]]\n",
    "            refs=[[p for j,p in enumerate(preds) if j!=i]]\n",
    "            scores.append(sacrebleu.corpus_bleu(hyp, refs).score)\n",
    "        sbleu = float(np.mean(scores))\n",
    "    return d2, d3, sbleu\n",
    "\n",
    "# --- MORE SENSITIVE near-duplicate detector  ---\n",
    "_ar_punct = r\"[^\\w\\s\\u0600-\\u06FF]\"\n",
    "_ar_tatweel = \"\\u0640\"\n",
    "_ar_diacritics = re.compile(r\"[\\u0610-\\u061A\\u064B-\\u065F\\u06D6-\\u06ED]\")\n",
    "\n",
    "def normalize_ar(text: str) -> str:\n",
    "    t = text\n",
    "    t = t.replace(_ar_tatweel, \"\")               \n",
    "    t = _ar_diacritics.sub(\"\", t)                  \n",
    "    t = re.sub(r\"\\s+\", \" \", t)                     \n",
    "    t = re.sub(_ar_punct, \" \", t)                  \n",
    "    return t.strip()\n",
    "\n",
    "def near_duplicate_rate(preds, thr=0.90):\n",
    "    if len(preds) < 2:\n",
    "        return 0.0\n",
    "    norm = [normalize_ar(p) for p in preds]\n",
    "    vec = TfidfVectorizer(analyzer=\"word\", ngram_range=(1,3), min_df=1)\n",
    "    X = vec.fit_transform(norm)\n",
    "    sims = cosine_similarity(X)\n",
    "    n = X.shape[0]; cnt = 0; denom = n*(n-1)/2\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            if sims[i, j] >= thr:\n",
    "                cnt += 1\n",
    "    return 100.0 * cnt / max(1, denom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554d3036",
   "metadata": {},
   "source": [
    "## Summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33407793-cc43-4c9b-a45b-f1e5e333b29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jais-13b-chat label counts: Counter({'MSA': 381, 'GLF': 241, 'LEV': 152, 'MAGHREB': 40, 'EGY': 22})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Baseline not Found for bert-base-multilingual-cased on ar at /workspace/.venv/lib/python3.11/site-packages/bert_score/rescale_baseline/ar/bert-base-multilingual-cased.tsv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== Summary ==\n",
      "['jais-13b-chat', 28.83, 44.27, 10.41, 0.96, 15.95, 0.6581, 0.6933, 0.8087, 0.35, 0.0]\n",
      "\n",
      "Saved: eval_external_jais_13b/summary_saudi_only.csv\n"
     ]
    }
   ],
   "source": [
    "# ---------- evaluate & summarize ----------\n",
    "summary=[]\n",
    "for pf in pred_files:  \n",
    "    name  = Path(pf).stem\n",
    "    rows  = read_jsonl(pf)\n",
    "    preds = [r[\"pred\"] for r in rows]\n",
    "    golds = [r[\"gold\"] for r in rows]\n",
    "\n",
    "    probs = camel_probs_batched(preds, bs=64)\n",
    "    labs  = labels_from_probs(probs)\n",
    "    conf  = probs.max(axis=1)\n",
    "\n",
    "    print(f\"{name} label counts:\", Counter(labs))  \n",
    "\n",
    "    # metrics\n",
    "    msa_leak   = float(np.mean(probs[:, MSA_IDX]))*100.0 if MSA_IDX is not None else 0.0\n",
    "    saudi_rate = 100.0*float(np.mean([is_saudi(x) for x in labs]))\n",
    "    low_conf   = 100.0*float(np.mean(conf < 0.55))\n",
    "    echo       = tag_echo_rate(preds)\n",
    "\n",
    "    chrf = sacrebleu.corpus_chrf(preds, [golds]).score\n",
    "    P,R,F = bertscore(preds, golds, lang=\"ar\", rescale_with_baseline=True)\n",
    "    bert_f1 = float(F.mean().item())\n",
    "\n",
    "    d2, d3, sbleu = diversity_metrics(preds)\n",
    "    near_dup = near_duplicate_rate(preds, thr=0.90)\n",
    "\n",
    "    with (OUT_DIR / f\"{name}_report.json\").open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\n",
    "            \"model\": name, \"n\": len(rows),\n",
    "            \"saudi_rate_pct\": saudi_rate,\n",
    "            \"msa_leak_pct\": msa_leak,\n",
    "            \"low_conf_pct\": low_conf,\n",
    "            \"tag_echo_pct\": echo,\n",
    "            \"chrF++\": chrf,\n",
    "            \"BERTScore_F1\": bert_f1,\n",
    "            \"distinct2\": d2, \"distinct3\": d3, \"selfBLEU\": sbleu,\n",
    "            \"near_duplicate_pct\": near_dup\n",
    "        }, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    summary.append([name, round(saudi_rate,2), round(msa_leak,2), round(low_conf,2),\n",
    "                    round(echo,2), round(chrf,2), round(bert_f1,4),\n",
    "                    round(d2,4), round(d3,4), round(sbleu,2), round(near_dup,2)])\n",
    "\n",
    "csv_path = OUT_DIR / \"summary_saudi_only.csv\"\n",
    "with csv_path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow([\"Model\",\"Saudi% (GLF) ↑\",\"MSA leak% ↓\",\"Low-conf% ↓\",\n",
    "                \"Tag-echo% ↓\",\"chrF++ ↑\",\"BERTScore F1 ↑\",\n",
    "                \"distinct-2 ↑\",\"distinct-3 ↑\",\"Self-BLEU ↓\",\"Near-dup% ↓\"])\n",
    "    for row in summary: w.writerow(row)\n",
    "\n",
    "print(\"\\n== Summary ==\")\n",
    "for row in summary: print(row)\n",
    "print(f\"\\nSaved: {csv_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "allam-eval",
   "language": "python",
   "name": "allam-eval"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
