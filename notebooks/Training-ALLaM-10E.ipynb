{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1dc868d",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964945e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6,546 items\n",
      "Train: 5,122\n",
      "  Hijazi:  2650  (51.74%)\n",
      "  Najdi :  2461  (48.05%)\n",
      "  HIJAZI:     5  ( 0.10%)\n",
      "  NAJDI :     6  ( 0.12%)\n",
      "Dev: 588\n",
      "  Hijazi:   304  (51.70%)\n",
      "  Najdi :   284  (48.30%)\n",
      "Test: 836\n",
      "  Hijazi:   413  (49.40%)\n",
      "  Najdi :   385  (46.05%)\n",
      "  HIJAZI:    20  ( 2.39%)\n",
      "  NAJDI :    18  ( 2.15%)\n",
      "Saved splits to data_splits\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json, random, math\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "IN_PATH   = Path(\"saudi_dataset_all.jsonl\")\n",
    "OUT_DIR   = Path(\"data_splits\")\n",
    "SPLIT_SEED = 42\n",
    "SPLITS = (0.80, 0.10, 0.10)  # train/dev/test\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def read_jsonl(p):\n",
    "    with p.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line=line.strip()\n",
    "            if line:\n",
    "                yield json.loads(line)\n",
    "\n",
    "def get_dialect(obj):\n",
    "    \n",
    "    if \"dialect\" in obj and obj[\"dialect\"]:\n",
    "        return str(obj[\"dialect\"]).strip()\n",
    "    meta = obj.get(\"meta\") or {}\n",
    "    return str(meta.get(\"dialect\",\"\")).strip()\n",
    "\n",
    "def get_topic(obj):\n",
    "    meta = obj.get(\"meta\") or {}\n",
    "    return str(meta.get(\"topic\",\"_NA_\")).strip()\n",
    "\n",
    "def get_length(obj):\n",
    "    meta = obj.get(\"meta\") or {}\n",
    "    return str(meta.get(\"length\",\"_NA_\")).strip()\n",
    "\n",
    "rows = list(read_jsonl(IN_PATH))\n",
    "print(f\"Loaded {len(rows):,} items\")\n",
    "\n",
    "def bucket_of(ex):\n",
    "    d = get_dialect(ex)\n",
    "    t = get_topic(ex)\n",
    "    L = get_length(ex)\n",
    "    return (d, t, L)\n",
    "\n",
    "groups = defaultdict(list)\n",
    "for ex in rows:\n",
    "    groups[bucket_of(ex)].append(ex)\n",
    "\n",
    "random.seed(SPLIT_SEED)\n",
    "train, dev, test = [], [], []\n",
    "for key, lst in groups.items():\n",
    "    random.shuffle(lst)\n",
    "    n = len(lst)\n",
    "    n_tr = math.floor(SPLITS[0]*n)\n",
    "    n_de = math.floor(SPLITS[1]*n)\n",
    "    train += lst[:n_tr]\n",
    "    dev   += lst[n_tr:n_tr+n_de]\n",
    "    test  += lst[n_tr+n_de:]\n",
    "\n",
    "def dump(path, data):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as out:\n",
    "        for r in data:\n",
    "            out.write(json.dumps({\n",
    "                \"instruction\": r.get(\"instruction\",\"\"),        \n",
    "                \"response\":    r.get(\"response\",\"\"),\n",
    "                \"dialect\":     get_dialect(r)                  \n",
    "            }, ensure_ascii=False)+\"\\n\")\n",
    "\n",
    "dump(OUT_DIR/\"train.jsonl\", train)\n",
    "dump(OUT_DIR/\"dev.jsonl\",   dev)\n",
    "dump(OUT_DIR/\"test.jsonl\",  test)\n",
    "\n",
    "def show_counts(name, rows_):\n",
    "    c = Counter(get_dialect(r) for r in rows_)\n",
    "    total = len(rows_) or 1\n",
    "    print(f\"{name}: {len(rows_):,}\")\n",
    "    for k,v in c.items():\n",
    "        print(f\"  {k:6s}: {v:5d}  ({100*v/total:5.2f}%)\")\n",
    "\n",
    "show_counts(\"Train\", train)\n",
    "show_counts(\"Dev\",   dev)\n",
    "show_counts(\"Test\",  test)\n",
    "print(\"Saved splits to\", OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf730f6",
   "metadata": {},
   "source": [
    "## Training ALLaM using LoRA + Tag, 10 EPOCH "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c3776f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ENVIRONMENT CHECK\n",
      "Python executable: /usr/bin/python\n",
      "Python version   : 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]\n",
      "Torch version    : 2.3.1+cu121\n",
      "Transformers ver : 4.42.3\n",
      "CUDA available   : True\n",
      "CUDA device count: 1\n",
      "  Device 0: NVIDIA RTX 6000 Ada Generation\n",
      "Datasets version : 2.19.1\n",
      "PEFT version     : 0.11.1\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54b4cb959b584b0bb4886491c7642933",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5122 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "992f1367bfd747d2bb548bad9429a9ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/588 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train examples: 5,122 | Dev examples: 588\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "360fd7f2b1f34925810362ed235a85c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 79,953,920 || all params: 7,080,513,536 || trainable%: 1.1292\n",
      "Starting trainingâ€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py:280: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='130' max='130' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [130/130 30:14, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.083500</td>\n",
       "      <td>2.590824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.473200</td>\n",
       "      <td>2.168015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.981300</td>\n",
       "      <td>2.007209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.889800</td>\n",
       "      <td>1.946524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.819900</td>\n",
       "      <td>1.914637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.725000</td>\n",
       "      <td>1.898293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.697000</td>\n",
       "      <td>1.893849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.644700</td>\n",
       "      <td>1.892510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.638700</td>\n",
       "      <td>1.892369</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 10] Epoch 0.72 | Train: N/A | Eval: N/A | Step: 133.36s | Elapsed: 2.22m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 20] Epoch 1.44 | Train: N/A | Eval: N/A | Step: 141.90s | Elapsed: 4.59m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 30] Epoch 2.16 | Train: N/A | Eval: N/A | Step: 142.33s | Elapsed: 6.96m\n",
      "[Step 40] Epoch 2.88 | Train: N/A | Eval: N/A | Step: 134.56s | Elapsed: 9.20m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 50] Epoch 3.60 | Train: N/A | Eval: N/A | Step: 142.07s | Elapsed: 11.57m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 60] Epoch 4.32 | Train: N/A | Eval: N/A | Step: 142.56s | Elapsed: 13.95m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 70] Epoch 5.05 | Train: N/A | Eval: N/A | Step: 142.22s | Elapsed: 16.32m\n",
      "[Step 80] Epoch 5.77 | Train: N/A | Eval: N/A | Step: 134.47s | Elapsed: 18.56m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 90] Epoch 6.49 | Train: N/A | Eval: N/A | Step: 141.72s | Elapsed: 20.92m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 100] Epoch 7.21 | Train: N/A | Eval: N/A | Step: 142.53s | Elapsed: 23.30m\n",
      "[Step 110] Epoch 7.93 | Train: N/A | Eval: N/A | Step: 134.48s | Elapsed: 25.54m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 120] Epoch 8.65 | Train: N/A | Eval: N/A | Step: 142.23s | Elapsed: 27.91m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 130] Epoch 9.37 | Train: N/A | Eval: N/A | Step: 142.69s | Elapsed: 30.29m\n",
      "âœ… Done. LoRA adapters saved to: outputs/allam7b-lora-token-10EPOCH\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final evaluation metrics: {'eval_loss': 1.892369270324707, 'eval_runtime': 6.8343, 'eval_samples_per_second': 3.658, 'eval_steps_per_second': 0.585, 'epoch': 9.36936936936937}\n",
      "Perplexity: 6.635\n",
      "Saved run_config.txt\n"
     ]
    }
   ],
   "source": [
    "import os, sys, json, time, random, numpy as np, torch, transformers\n",
    "from pathlib import Path\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainerCallback\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from peft import LoraConfig\n",
    "from transformers import set_seed\n",
    "\n",
    "\n",
    "os.environ.setdefault(\"CUDA_VISIBLE_DEVICES\", \"0\")\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
    "torch.cuda.empty_cache()\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ENVIRONMENT CHECK\")\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"Python version   : {sys.version}\")\n",
    "print(f\"Torch version    : {torch.__version__}\")\n",
    "print(f\"Transformers ver : {transformers.__version__}\")\n",
    "print(f\"CUDA available   : {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  Device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"No CUDA device detected!\")\n",
    "try:\n",
    "    import datasets; print(f\"Datasets version : {datasets.__version__}\")\n",
    "except Exception: print(\"Datasets not installed?\")\n",
    "try:\n",
    "    import peft; print(f\"PEFT version     : {peft.__version__}\")\n",
    "except Exception: print(\"PEFT not installed?\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "TRAIN_PATH = Path(\"data_splits/train.jsonl\")\n",
    "DEV_PATH   = Path(\"data_splits/dev.jsonl\")\n",
    "OUTPUT_DIR = Path(\"outputs/allam7b-lora-token-10EPOCH\")\n",
    "BASE_MODEL = \"ALLaM-AI/ALLaM-7B-Instruct-preview\"\n",
    "MAX_SEQ_LEN = 2048  \n",
    "\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=32,  \n",
    "    lora_alpha=64,  \n",
    "    lora_dropout=0.1, \n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"]\n",
    ")\n",
    "\n",
    "\n",
    "train_args = SFTConfig(\n",
    "    output_dir=str(OUTPUT_DIR),\n",
    "    num_train_epochs=10,  \n",
    "    per_device_train_batch_size=2,  \n",
    "    gradient_accumulation_steps=8,  \n",
    "    learning_rate=5e-5,  \n",
    "    lr_scheduler_type=\"cosine_with_restarts\",\n",
    "    warmup_ratio=0.1,  \n",
    "    bf16=True,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    gradient_checkpointing=True,\n",
    "    report_to=\"tensorboard\",\n",
    "    save_total_limit=3,  \n",
    "    seed=SEED,\n",
    "    packing=True,\n",
    "    dataset_text_field=\"text\",\n",
    ")\n",
    "\n",
    "def load_jsonl(path: Path) -> Dataset:\n",
    "    rows = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if not line.strip(): continue\n",
    "            obj = json.loads(line)\n",
    "            instr = (obj.get(\"instruction\") or \"\").strip()\n",
    "            resp  = (obj.get(\"response\") or \"\").strip()\n",
    "            if instr and resp:\n",
    "                rows.append({\"instruction\": instr, \"response\": resp})\n",
    "    if not rows:\n",
    "        raise ValueError(f\"No valid rows found in {path}\")\n",
    "    return Dataset.from_list(rows)\n",
    "\n",
    "train_ds = load_jsonl(TRAIN_PATH)\n",
    "dev_ds   = load_jsonl(DEV_PATH)\n",
    "\n",
    "EOS = \"</s>\"\n",
    "def fmt(example):\n",
    "    instr = example[\"instruction\"]  \n",
    "    resp  = example[\"response\"]\n",
    "    text = f\"### Instruction:\\n{instr}\\n\\n### Response:\\n{resp}{EOS}\"\n",
    "    return {\"text\": text}\n",
    "\n",
    "train_text = train_ds.map(fmt, remove_columns=[c for c in train_ds.column_names if c!=\"text\"])\n",
    "dev_text   = dev_ds.map(fmt,   remove_columns=[c for c in dev_ds.column_names if c!=\"text\"])\n",
    "\n",
    "print(f\"Train examples: {len(train_text):,} | Dev examples: {len(dev_text):,}\")\n",
    "\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.config.use_cache = False\n",
    "\n",
    "\n",
    "class LoggingCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.t0 = time.time()\n",
    "        self.tlast = self.t0\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kw):\n",
    "        if not state.is_local_process_zero:\n",
    "            return\n",
    "        logs = logs or {}\n",
    "        now = time.time()\n",
    "        tl = logs.get(\"loss\")\n",
    "        el = logs.get(\"eval_loss\")\n",
    "    \n",
    "        if \"loss\" in logs or \"eval_loss\" in logs or \"learning_rate\" in logs:\n",
    "            print(\n",
    "                f\"[Step {state.global_step}] epoch={state.epoch:.2f} \"\n",
    "                f\"train_loss={tl if tl is not None else 'N/A'} \"\n",
    "                f\"eval_loss={el if el is not None else 'N/A'} \"\n",
    "                f\"lr={logs.get('learning_rate','?')} \"\n",
    "                f\"dt={now-self.tlast:.2f}s elapsed={(now-self.t0)/60:.2f}m\",\n",
    "                flush=True\n",
    "            )\n",
    "            self.tlast = now\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tok,  \n",
    "    train_dataset=train_text,\n",
    "    eval_dataset=dev_text,\n",
    "    peft_config=lora_config,\n",
    "    args=train_args,\n",
    "    max_seq_length=MAX_SEQ_LEN, \n",
    ")\n",
    "\n",
    "\n",
    "try:\n",
    "    trainer.model.print_trainable_parameters()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "trainer.add_callback(LoggingCallback())\n",
    "\n",
    "print(\"Starting trainingâ€¦\", flush=True)\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "trainer.save_model()\n",
    "tok.save_pretrained(OUTPUT_DIR)\n",
    "print(\"âœ… Done. LoRA adapters saved to:\", OUTPUT_DIR)\n",
    "\n",
    "\n",
    "metrics = trainer.evaluate()\n",
    "print(\"Final evaluation metrics:\", metrics)\n",
    "if \"eval_loss\" in metrics and metrics[\"eval_loss\"] is not None:\n",
    "    try:\n",
    "        ppl = float(np.exp(metrics[\"eval_loss\"]))\n",
    "        print(f\"Perplexity: {ppl:.3f}\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "with open(OUTPUT_DIR / \"run_config.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"Seed: {SEED}\\n\")\n",
    "    f.write(f\"Base model: {BASE_MODEL}\\n\")\n",
    "    f.write(f\"Train path: {TRAIN_PATH}\\n\")\n",
    "    f.write(f\"Dev path  : {DEV_PATH}\\n\")\n",
    "    f.write(f\"Epochs: {train_args.num_train_epochs}\\n\")\n",
    "    f.write(f\"LR: {train_args.learning_rate}\\n\")\n",
    "    f.write(f\"Per-device batch: {train_args.per_device_train_batch_size}\\n\")\n",
    "    f.write(f\"Grad accum: {train_args.gradient_accumulation_steps}\\n\")\n",
    "    f.write(f\"Max seq len: {MAX_SEQ_LEN}\\n\")\n",
    "    f.write(f\"BF16: {train_args.bf16}\\n\")\n",
    "    f.write(f\"LORA r/alpha/drop: {lora_config.r}/{lora_config.lora_alpha}/{lora_config.lora_dropout}\\n\")\n",
    "    f.write(f\"Transformers: {transformers.__version__}\\n\")\n",
    "    f.write(f\"Torch: {torch.__version__}\\n\")\n",
    "print(\"Saved run_config.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1f345d",
   "metadata": {},
   "source": [
    "## Training ALLaM using LoRA + No Tag, 10EPOCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1a4190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "NO-TAG TRAIN â€” ENVIRONMENT CHECK\n",
      "Python executable: /usr/bin/python\n",
      "Python version   : 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]\n",
      "Torch version    : 2.3.1+cu121\n",
      "Transformers ver : 4.42.3\n",
      "CUDA available   : True\n",
      "CUDA device count: 1\n",
      "  Device 0: NVIDIA RTX 6000 Ada Generation\n",
      "Datasets version : 2.19.1\n",
      "PEFT version     : 0.11.1\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44a2012e73a44ef288e6ccf569727a11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5122 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4650d7d4d24478b9c2613e037697ead",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/588 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train examples: 5,122 | Dev examples: 588\n",
      "Leading <DIALECT=â€¦> tags removed in formatting: 5710/5710 (100.00%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b997e3dc5a274e9e931b52f914c2e76b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 79,953,920 || all params: 7,080,513,536 || trainable%: 1.1292\n",
      "Starting NO-TAG trainingâ€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py:280: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/120 28:02, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.398500</td>\n",
       "      <td>2.890326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.731300</td>\n",
       "      <td>2.469382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.449100</td>\n",
       "      <td>2.253285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.208700</td>\n",
       "      <td>2.182941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.026000</td>\n",
       "      <td>2.142709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.956800</td>\n",
       "      <td>2.123169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.929200</td>\n",
       "      <td>2.115768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.864800</td>\n",
       "      <td>2.111516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.855700</td>\n",
       "      <td>2.111593</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 10] epoch=0.81 train_loss=3.3985 eval_loss=N/A lr=4.166666666666667e-05 dt=133.32s elapsed=2.22m\n",
      "[Step 12] epoch=0.97 train_loss=N/A eval_loss=2.890326499938965 lr=? dt=38.37s elapsed=2.86m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 20] epoch=1.62 train_loss=2.7313 eval_loss=N/A lr=4.9326121764495596e-05 dt=104.38s elapsed=4.60m\n",
      "[Step 24] epoch=1.94 train_loss=N/A eval_loss=2.4693820476531982 lr=? dt=70.34s elapsed=5.77m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 30] epoch=2.42 train_loss=2.4491 eval_loss=N/A lr=4.665063509461097e-05 dt=72.41s elapsed=6.98m\n",
      "[Step 37] epoch=2.99 train_loss=N/A eval_loss=2.2532846927642822 lr=? dt=102.31s elapsed=8.69m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 40] epoch=3.23 train_loss=2.2087 eval_loss=N/A lr=4.215604094671835e-05 dt=40.23s elapsed=9.36m\n",
      "[Step 49] epoch=3.96 train_loss=N/A eval_loss=2.182940721511841 lr=? dt=134.32s elapsed=11.59m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 50] epoch=4.04 train_loss=2.0941 eval_loss=N/A lr=3.621997950501156e-05 dt=8.38s elapsed=11.73m\n",
      "[Step 60] epoch=4.85 train_loss=2.026 eval_loss=N/A lr=2.9341204441673266e-05 dt=134.76s elapsed=13.98m\n",
      "[Step 61] epoch=4.93 train_loss=N/A eval_loss=2.142709493637085 lr=? dt=31.66s elapsed=14.51m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 70] epoch=5.66 train_loss=1.9568 eval_loss=N/A lr=2.2097677146869242e-05 dt=110.83s elapsed=16.36m\n",
      "[Step 74] epoch=5.98 train_loss=N/A eval_loss=2.123169183731079 lr=? dt=63.75s elapsed=17.42m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 80] epoch=6.46 train_loss=1.9292 eval_loss=N/A lr=1.509800584902108e-05 dt=79.14s elapsed=18.74m\n",
      "[Step 86] epoch=6.95 train_loss=N/A eval_loss=2.1157681941986084 lr=? dt=95.68s elapsed=20.33m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 90] epoch=7.27 train_loss=1.8762 eval_loss=N/A lr=8.930309757836517e-06 dt=46.77s elapsed=21.11m\n",
      "[Step 99] epoch=8.00 train_loss=N/A eval_loss=2.111159086227417 lr=? dt=127.52s elapsed=23.24m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 100] epoch=8.08 train_loss=1.8661 eval_loss=N/A lr=4.112804714676594e-06 dt=15.01s elapsed=23.49m\n",
      "[Step 110] epoch=8.89 train_loss=1.8648 eval_loss=N/A lr=1.0502621921127776e-06 dt=134.57s elapsed=25.73m\n",
      "[Step 111] epoch=8.97 train_loss=N/A eval_loss=2.111515760421753 lr=? dt=24.93s elapsed=26.14m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 120] epoch=9.70 train_loss=1.8557 eval_loss=N/A lr=0.0 dt=117.92s elapsed=28.11m\n",
      "[Step 120] epoch=9.70 train_loss=N/A eval_loss=2.11159348487854 lr=? dt=8.08s elapsed=28.24m\n",
      "âœ… Done. LoRA (NO TAG) adapters saved to: outputs/allam7b-lora-no-token-10EPOCH\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 120] epoch=9.70 train_loss=N/A eval_loss=2.11159348487854 lr=? dt=8.90s elapsed=28.39m\n",
      "Final evaluation metrics: {'eval_loss': 2.11159348487854, 'eval_runtime': 6.34, 'eval_samples_per_second': 3.628, 'eval_steps_per_second': 0.473, 'epoch': 9.696969696969697}\n",
      "Perplexity: 8.261\n",
      "Saved run_config.txt\n"
     ]
    }
   ],
   "source": [
    "TRAIN_PATH = Path(\"data_splits/train.jsonl\")\n",
    "DEV_PATH   = Path(\"data_splits/dev.jsonl\")\n",
    "OUTPUT_DIR = Path(\"outputs/allam7b-lora-no-token-10EPOCH\")\n",
    "BASE_MODEL = \"ALLaM-AI/ALLaM-7B-Instruct-preview\"\n",
    "MAX_SEQ_LEN = 2048\n",
    "\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=32,  #\n",
    "    lora_alpha=64,  \n",
    "    lora_dropout=0.1,  \n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"]\n",
    ")\n",
    "\n",
    "\n",
    "train_args = SFTConfig(\n",
    "    output_dir=str(OUTPUT_DIR),\n",
    "    num_train_epochs=10, \n",
    "    per_device_train_batch_size=2,  \n",
    "    gradient_accumulation_steps=8, \n",
    "    learning_rate=5e-5, \n",
    "    lr_scheduler_type=\"cosine_with_restarts\",\n",
    "    warmup_ratio=0.1, \n",
    "    bf16=True,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    gradient_checkpointing=True,\n",
    "    report_to=\"tensorboard\",\n",
    "    save_total_limit=3, \n",
    "    seed=SEED,\n",
    "    packing=True,\n",
    "    dataset_text_field=\"text\",\n",
    ")\n",
    "\n",
    "\n",
    "def load_jsonl(path: Path) -> Dataset:\n",
    "    rows = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            obj = json.loads(line)\n",
    "            instr = (obj.get(\"instruction\") or \"\").strip()\n",
    "            resp  = (obj.get(\"response\") or \"\").strip()\n",
    "            if instr and resp:\n",
    "                rows.append({\"instruction\": instr, \"response\": resp})\n",
    "    if not rows:\n",
    "        raise ValueError(f\"No valid rows found in {path}\")\n",
    "    return Dataset.from_list(rows)\n",
    "\n",
    "train_ds = load_jsonl(TRAIN_PATH)\n",
    "dev_ds   = load_jsonl(DEV_PATH)\n",
    "\n",
    "TAG_RE = re.compile(r'^\\s*<\\s*DIALECT\\s*=\\s*(HIJAZI|NAJDI)\\s*>\\s*', re.IGNORECASE)\n",
    "EOS = \"</s>\"\n",
    "\n",
    "removed_ct = 0\n",
    "total_ct   = 0\n",
    "def fmt_no_tag(example):\n",
    "    global removed_ct, total_ct\n",
    "    instr = example[\"instruction\"]\n",
    "    total_ct += 1\n",
    "    stripped = TAG_RE.sub(\"\", instr).lstrip()\n",
    "    if stripped != instr:\n",
    "        removed_ct += 1\n",
    "    resp = example[\"response\"]\n",
    "    text = f\"### Instruction:\\n{stripped}\\n\\n### Response:\\n{resp}{EOS}\"\n",
    "    return {\"text\": text}\n",
    "\n",
    "train_text = train_ds.map(fmt_no_tag, remove_columns=[c for c in train_ds.column_names if c!=\"text\"])\n",
    "dev_text   = dev_ds.map(fmt_no_tag,   remove_columns=[c for c in dev_ds.column_names if c!=\"text\"])\n",
    "\n",
    "print(f\"Train examples: {len(train_text):,} | Dev examples: {len(dev_text):,}\")\n",
    "print(f\"Leading <DIALECT=â€¦> tags removed in formatting: {removed_ct}/{total_ct} \"\n",
    "      f\"({(removed_ct/max(1,total_ct))*100:.2f}%)\")\n",
    "\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "\n",
    "class LoggingCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.t0 = time.time()\n",
    "        self.tlast = self.t0\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kw):\n",
    "        if not state.is_local_process_zero:\n",
    "            return\n",
    "        logs = logs or {}\n",
    "        now = time.time()\n",
    "        tl = logs.get(\"loss\")\n",
    "        el = logs.get(\"eval_loss\")\n",
    "        if \"loss\" in logs or \"eval_loss\" in logs or \"learning_rate\" in logs:\n",
    "            print(\n",
    "                f\"[Step {state.global_step}] epoch={state.epoch:.2f} \"\n",
    "                f\"train_loss={tl if tl is not None else 'N/A'} \"\n",
    "                f\"eval_loss={el if el is not None else 'N/A'} \"\n",
    "                f\"lr={logs.get('learning_rate','?')} \"\n",
    "                f\"dt={now-self.tlast:.2f}s elapsed={(now-self.t0)/60:.2f}m\",\n",
    "                flush=True\n",
    "            )\n",
    "            self.tlast = now\n",
    "\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tok,  \n",
    "    train_dataset=train_text,\n",
    "    eval_dataset=dev_text,\n",
    "    peft_config=lora_config,\n",
    "    args=train_args,\n",
    "    max_seq_length=MAX_SEQ_LEN,  \n",
    ")\n",
    "\n",
    "\n",
    "try:\n",
    "    trainer.model.print_trainable_parameters()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "trainer.add_callback(LoggingCallback())\n",
    "print(\"Starting NO-TAG trainingâ€¦\", flush=True)\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "trainer.save_model()\n",
    "tok.save_pretrained(OUTPUT_DIR)\n",
    "print(\"âœ… Done. LoRA (NO TAG) adapters saved to:\", OUTPUT_DIR)\n",
    "\n",
    "metrics = trainer.evaluate()\n",
    "print(\"Final evaluation metrics:\", metrics)\n",
    "if \"eval_loss\" in metrics and metrics[\"eval_loss\"] is not None:\n",
    "    try:\n",
    "        ppl = float(np.exp(metrics[\"eval_loss\"]))\n",
    "        print(f\"Perplexity: {ppl:.3f}\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "with open(OUTPUT_DIR / \"run_config.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"Seed: {SEED}\\n\")\n",
    "    f.write(f\"Base model: {BASE_MODEL}\\n\")\n",
    "    f.write(f\"Train path: {TRAIN_PATH}\\n\")\n",
    "    f.write(f\"Dev path  : {DEV_PATH}\\n\")\n",
    "    f.write(f\"Epochs: {train_args.num_train_epochs}\\n\")\n",
    "    f.write(f\"LR: {train_args.learning_rate}\\n\")\n",
    "    f.write(f\"Per-device batch: {train_args.per_device_train_batch_size}\\n\")\n",
    "    f.write(f\"Grad accum: {train_args.gradient_accumulation_steps}\\n\")\n",
    "    f.write(f\"Max seq len: {MAX_SEQ_LEN}\\n\")\n",
    "    f.write(f\"BF16: {train_args.bf16}\\n\")\n",
    "    f.write(f\"LORA r/alpha/drop: {lora_config.r}/{lora_config.lora_alpha}/{lora_config.lora_dropout}\\n\")\n",
    "    f.write(f\"Transformers: {transformers.__version__}\\n\")\n",
    "    f.write(f\"Torch: {torch.__version__}\\n\")\n",
    "print(\"Saved run_config.txt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
