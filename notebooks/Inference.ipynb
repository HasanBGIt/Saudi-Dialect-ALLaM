{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245208ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, torch, gc\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "BASE_MODEL      = \"ALLaM-AI/ALLaM-7B-Instruct-preview\"\n",
    "ADAPTER_TOKEN   = \"outputs/allam7b-lora-token-15EPOCH/checkpoint-97\"\n",
    "ADAPTER_NOTOKEN = \"outputs/allam7b-lora-no-token-15EPOCH/checkpoint-86\"\n",
    "\n",
    "TAG_RE = re.compile(r'^\\s*<\\s*DIALECT\\s*=\\s*(HIJAZI|NAJDI)\\s*>\\s*', re.IGNORECASE)\n",
    "\n",
    "def add_tag(instruction, dialect=None):\n",
    "    if not dialect:\n",
    "        return instruction\n",
    "    d = str(dialect).strip().lower()\n",
    "    if d.startswith(\"hij\"):\n",
    "        tag = \"<DIALECT=HIJAZI>\"\n",
    "    elif d.startswith(\"naj\"):\n",
    "        tag = \"<DIALECT=NAJDI>\"\n",
    "    else:\n",
    "        return instruction\n",
    "    if TAG_RE.match(instruction or \"\"):\n",
    "        return instruction\n",
    "    return f\"{tag}{(instruction or '').lstrip()}\"\n",
    "\n",
    "def strip_tag(instruction):\n",
    "    return TAG_RE.sub(\"\", instruction or \"\").lstrip()\n",
    "\n",
    "def _load_tokenizer():\n",
    "    tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
    "    if tok.pad_token is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "    return tok\n",
    "\n",
    "def load_base():\n",
    "    tok = _load_tokenizer()\n",
    "    dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float16\n",
    "    mdl = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL, torch_dtype=dtype, device_map=\"auto\"\n",
    "    )\n",
    "    mdl.config.use_cache = True\n",
    "    mdl.eval()\n",
    "    return tok, mdl\n",
    "\n",
    "def load_with_lora(adapter_dir):\n",
    "    tok = _load_tokenizer()\n",
    "    dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float16\n",
    "    base = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL, torch_dtype=dtype, device_map=\"auto\"\n",
    "    )\n",
    "    base.config.use_cache = True\n",
    "    mdl = PeftModel.from_pretrained(base, adapter_dir)\n",
    "    mdl.eval()\n",
    "    return tok, mdl\n",
    "\n",
    "def prove_lora(model):\n",
    "    is_peft = isinstance(model, PeftModel)\n",
    "    try:\n",
    "        adapters = list(model.peft_config.keys())\n",
    "    except Exception:\n",
    "        adapters = []\n",
    "    lora_modules = [n for n, _ in model.named_modules() if \"lora_\" in n.lower()]\n",
    "    sample = lora_modules[:8]\n",
    "    return {\n",
    "        \"is_peft_model\": is_peft,\n",
    "        \"active_adapters\": adapters,\n",
    "        \"sample_lora_modules\": sample,\n",
    "        \"n_lora_modules\": len(lora_modules),\n",
    "    }\n",
    "\n",
    "def build_prompt(instruction):\n",
    "    return f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n\"\n",
    "\n",
    "@torch.no_grad()  \n",
    "def generate(model, tokenizer, instruction, *,\n",
    "             max_new_tokens=160, temperature=0.7, top_p=0.95, top_k=50, repetition_penalty=1.05):\n",
    "    prompt = build_prompt(instruction)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    out = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    txt = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    return txt.split(\"### Response:\\n\", 1)[-1].strip()\n",
    "\n",
    "\n",
    "def clear_mem():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eacb2af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3f76e10a9de4d46be41ed724fc3ab01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Effective instructions\n",
      "- BASE     : <DIALECT=NAJDI>تكلم عن اول يوم دراسي لك في المدرسة\n",
      "- TOKEN    : <DIALECT=NAJDI>تكلم عن اول يوم دراسي لك في المدرسة\n",
      "- NO-TOKEN : تكلم عن اول يوم دراسي لك في المدرسة\n",
      "\n",
      "### Outputs\n",
      "[BASE]\n",
      "السلام عليكم ورحمة الله وبركاته،\n",
      "\n",
      "بسم الله الرحمن الرحيم، الحمد لله رب العالمين، والصلاة والسلام على أشرف الأنبياء والمرسلين، نبينا محمد وعلى آله وصحبه أجمعين.\n",
      "\n",
      "أما بعد، فإن أول يوم دراسي في المدرسة له مكانة خاصة في حياة كل طالب وطالبة، حيث يبدأون رحلة جديدة مليئة بالتعلم والمعرفة. هذا اليوم يمثل بداية مشوارهم الأكاديمي الذي سيؤثر بشكل كبير على حياتهم المستقبلية.\n",
      "\n",
      "في ذلك اليوم، كنت أشعر بمزيج من الحماس والقلق، فالحماس كان بسبب بدء مرحلة جديدة من حياتي الأكاديمية، بينما القلق كان بشأن كيفية التأقلم مع الأجواء الدراسية الجديدة. ولكن بفضل دعم والديّ وإدارتي المدرسية، تمكنت من تجاوز هذه المشاعر بسرعة وبدأت أستمتع بتجربتي الدراسية.\n",
      "\n",
      "منذ اليوم الأول، وجدت نفسي محاطًا بزملاء جدد ومعلمين أك\n",
      "\n",
      "[TOKEN]\n",
      "أول يوم لي كان شعور متوتر ومفرح، دخلت الصف لقيت العيال كلهم، ومع الوقت صار الجو أحلى وما حسّيت بالوقت.\n",
      "\n",
      "[NO-TOKEN]\n",
      "أول يوم بالمدرسة كان يوم مهم، دحين كنت متحمس ألبس البدلة وأروح مع أبوي، لما دخلت الصف لقيت مدرسين جدد وزملاء جدد، وكان الجو كله حماس، بس شوي خفت أول ما بدأت الحصة. بعدين صرت أحب المدرسة أكثر لما عرفت الأصدقاء وصرت أتشارك معهم الواجبات والألعاب.\n",
      "\n",
      "### LoRA attach proofs\n",
      "TOKEN   : {'is_peft_model': True, 'active_adapters': ['token'], 'sample_lora_modules': ['base_model.model.model.layers.0.self_attn.q_proj.lora_dropout', 'base_model.model.model.layers.0.self_attn.q_proj.lora_dropout.token', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.token', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.token', 'base_model.model.model.layers.0.self_attn.q_proj.lora_embedding_A', 'base_model.model.model.layers.0.self_attn.q_proj.lora_embedding_B'], 'n_lora_modules': 1792}\n",
      "NO-TOKEN: {'is_peft_model': True, 'active_adapters': ['token', 'no_token'], 'sample_lora_modules': ['base_model.model.model.layers.0.self_attn.q_proj.lora_dropout', 'base_model.model.model.layers.0.self_attn.q_proj.lora_dropout.token', 'base_model.model.model.layers.0.self_attn.q_proj.lora_dropout.no_token', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.token', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.no_token', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.token'], 'n_lora_modules': 2464}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "DIALECT     = \"najdi\"   \n",
    "INSTRUCTION = \"تكلم عن اول يوم دراسي لك في المدرسة\"\n",
    "GEN_KW = dict(max_new_tokens=160, temperature=0.7, top_p=0.95, top_k=50, repetition_penalty=1.05)\n",
    "\n",
    "def run_all(instruction, dialect=None, gen_kw=None):\n",
    "    gen_kw = gen_kw or {}\n",
    "    results = {}\n",
    "\n",
    "    # 1) Load tokenizer and BASE model once\n",
    "    tok = _load_tokenizer()\n",
    "    dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float16\n",
    "    base = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL, torch_dtype=dtype, device_map=\"auto\"\n",
    "    )\n",
    "    base.config.use_cache = True\n",
    "    base.eval()\n",
    "\n",
    "    # 2) BASE output (can add a tag to gently nudge style if dialect is set)\n",
    "    instr_base = add_tag(instruction, dialect) if dialect else instruction\n",
    "    results[\"base_instruction_eff\"] = instr_base\n",
    "    results[\"base_output\"] = generate(base, tok, instr_base, **gen_kw)\n",
    "\n",
    "    # 3) TOKEN LoRA (ensure tag is present)\n",
    "    peft_m = PeftModel.from_pretrained(base, ADAPTER_TOKEN, adapter_name=\"token\")\n",
    "    peft_m.eval()\n",
    "    instr_token = add_tag(instruction, dialect or \"najdi\")  \n",
    "    results[\"token_instruction_eff\"] = instr_token\n",
    "    results[\"token_proof\"] = prove_lora(peft_m)\n",
    "    results[\"token_output\"] = generate(peft_m, tok, instr_token, **gen_kw)\n",
    "\n",
    "    # 4) NO-TOKEN LoRA (ensure NO tag is in the prompt)\n",
    "    peft_m.load_adapter(ADAPTER_NOTOKEN, adapter_name=\"no_token\")\n",
    "    peft_m.set_adapter(\"no_token\")\n",
    "    instr_notoken = strip_tag(instruction)\n",
    "    results[\"no_token_instruction_eff\"] = instr_notoken\n",
    "    results[\"no_token_proof\"] = prove_lora(peft_m)\n",
    "    results[\"no_token_output\"] = generate(peft_m, tok, instr_notoken, **gen_kw)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "clear_mem()\n",
    "res = run_all(INSTRUCTION, DIALECT, GEN_KW)\n",
    "\n",
    "print(\"### Effective instructions\")\n",
    "print(\"- BASE     :\", res['base_instruction_eff'])\n",
    "print(\"- TOKEN    :\", res['token_instruction_eff'])\n",
    "print(\"- NO-TOKEN :\", res['no_token_instruction_eff'])\n",
    "print()\n",
    "\n",
    "print(\"### Outputs\")\n",
    "print(\"[BASE]\\n\", res['base_output'], \"\\n\", sep=\"\")\n",
    "print(\"[TOKEN]\\n\", res['token_output'], \"\\n\", sep=\"\")\n",
    "print(\"[NO-TOKEN]\\n\", res['no_token_output'], sep=\"\")\n",
    "\n",
    "print(\"\\n### LoRA attach proofs\")\n",
    "print(\"TOKEN   :\", res['token_proof'])\n",
    "print(\"NO-TOKEN:\", res['no_token_proof'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8cd39b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5006cab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb238cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a0daca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce6cb8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
